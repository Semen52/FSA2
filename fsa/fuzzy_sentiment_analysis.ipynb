{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ тональности текста с помощью нечеткой кластеризации в дистрибутивной семантике"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Программа демонстрирует применение алгоритмов четкой **k-means (KM)** и нечеткой **c-means (FCM)** кластеризации в дистрибутивной семантике для определения тональности высказываний. Рассматриваются анонимные русскоязычные высказывания из социальной сети Twitter (твиттер). В качестве примеров дистрибутивной семантики, используются модели, заранее обученные с помощью алгоритма **word2vec (skip-gram)**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первом эксперименте рассматривается подход, который был описан на площадке для анализа данных Kaggle (Источник: https://www.kaggle.com/c/word2vec-nlp-tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(863L, 300L)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "num_features = 300\n",
    "model = Word2Vec.load(\"models/300features_40minwords_10context_full\")\n",
    "# Вектора слов\n",
    "word_vectors = model.syn0\n",
    "# Список слов\n",
    "words = model.index2word\n",
    "\n",
    "print word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "import pandas as pd\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "#     featureVec = np.divide(featureVec,nwords)\n",
    "    if (nwords != 0):\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    else:\n",
    "        featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    " \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=True):\n",
    "        review = review.decode(\"utf-8\").split()\n",
    "#         print review\n",
    "#         exit(0)\n",
    "        return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 2541\n",
      "Review 1000 of 2541\n",
      "Review 2000 of 2541\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 1250\n",
      "Review 1000 of 1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Semen\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:50: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "train = pd.read_csv('data/stemmed/ttk_train_mystem.tsv',\n",
    "                    header=0,\n",
    "                    delimiter=\"\\t\",\n",
    "                    quoting=3)\n",
    "train = train[~train.sentiment.str.contains('neutral')]\n",
    "test = pd.read_csv('data/stemmed/ttk_test_etalon_mystem.tsv',\n",
    "                   header=0,\n",
    "                   delimiter=\"\\t\",\n",
    "                   quoting=3)\n",
    "test = test[~test.sentiment.str.contains('neutral')]\n",
    "\n",
    "# print train[:5]\n",
    "# print test[:5]\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train['text']:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n",
    "\n",
    "print \"Creating average feature vecs for test reviews\"\n",
    "clean_test_reviews = []\n",
    "for review in test['text']:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.77      0.92      0.84       879\n",
      "   positive       0.66      0.36      0.46       371\n",
      "\n",
      "avg / total       0.74      0.75      0.73      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit(trainDataVecs, train[\"sentiment\"])\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict(testDataVecs)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"sentiment\":test[\"sentiment\"], \"predicted\":result, \"text\":test[\"text\"]})\n",
    "output.to_csv(\"Word2Vec_AverageVectors.csv\", index=False, quoting=3)\n",
    "\n",
    "print classification_report(test[\"sentiment\"], result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  7.69099998474 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip(words, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Кластер 0\n",
      "повышать цена \n",
      "\n",
      "Кластер 1\n",
      ":) приходиться мама постоянно давно билайновский забывать роутер ни менять @sberbank платный \n",
      "\n",
      "Кластер 2\n",
      "рамка автомобиль 25 беларусь банковский альфа-банк денежный участник подписывать предоставление \n",
      "\n",
      "Кластер 3\n",
      "погашение условие наличный наличные отзыв расчет екатеринбург рассчитать авто \n",
      "\n",
      "Кластер 4\n",
      "друг у тут вчера поздравлять нет тот сегодня работать все \n",
      "\n",
      "Кластер 5\n",
      "филиал яновость разделять сми: оск финансовый намерен рейтинг #бизнес против расширять правительство россельхозбанк: журнал объединять вэб \n",
      "\n",
      "Кластер 6\n",
      "переставать отключать киевстар # \n",
      "\n",
      "Кластер 7\n",
      "плохой использовать ошибка крутой находиться должный ;) @borisnemtsov долго подключаться \n",
      "\n",
      "Кластер 8\n",
      "скачать \n",
      "\n",
      "Кластер 9\n",
      "проверять присылать тело \n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in xrange(0,10):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print \"\\nКластер %d\" % cluster\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    cluster_words = []\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if(word_centroid_map.values()[i] == cluster):\n",
    "            cluster_words.append(word_centroid_map.keys()[i])\n",
    "            print word_centroid_map.keys()[i],\n",
    "    \n",
    "    print ''\n",
    "#     print cluster_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros((train[\"text\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros((test[\"text\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.78      0.88      0.83       879\n",
      "   positive       0.59      0.40      0.48       371\n",
      "\n",
      "avg / total       0.72      0.74      0.72      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest and extract predictions \n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"sentiment\":test[\"sentiment\"], \"predicted\":result, \"text\":test[\"text\"]})\n",
    "output.to_csv(\"BagOfCentroids.csv\", index=False, quoting=3)\n",
    "\n",
    "print classification_report(test[\"sentiment\"], result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
